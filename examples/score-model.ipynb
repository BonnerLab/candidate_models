{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score your model, simply wrap it in one of the available wrappers (PyTorch, TensorFlow or Keras) and pass it to a scoring method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple convolution-relu-linear-relu model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from candidate_models.models.implementations.pytorch import PytorchModel\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        linear_input_size = np.power((224 - 3 + 2 * 0) / 1 + 1, 2) * 2\n",
    "        self.linear = torch.nn.Linear(int(linear_input_size), 1000)\n",
    "        self.relu2 = torch.nn.ReLU()  # can't get named ReLU output otherwise\n",
    "\n",
    "        # init weights for reproducibility\n",
    "        self.conv1.weight.data.fill_(0.01)\n",
    "        self.conv1.bias.data.fill_(0.01)\n",
    "        self.linear.weight.data.fill_(0.01)\n",
    "        self.linear.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, wrap the model in the PyTorch wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from candidate_models.models.implementations.pytorch import PytorchModel\n",
    "\n",
    "\n",
    "class MyModelWrapper(PytorchModel):\n",
    "    def _create_model(self, weights):\n",
    "        my_model = MyModel()\n",
    "        assert weights is None  # weight loading would go here\n",
    "        return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass the model to the scoring method and pass an identifier (for caching results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(_variable=<xarray.Variable (aggregation: 1)>\narray([0.247097])\nAttributes:\n    raw:      Score(_variable=<xarray.Variable (benchmark: 2, layer: 2, aggre...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 1)>\narray(['center'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "from candidate_models import score_model\n",
    "\n",
    "score = score_model(model_identifier='test_pytorch', model=MyModelWrapper, \n",
    "                    layers=['linear', 'relu2'], weights=None, pca_components=None)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns an aggregate Brain-Score of multiple benchmarks.\n",
    "For scores on individual benchmarks, see the \"Score\" section at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple model with a convolution and pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def _create_tf_model(inputs):\n",
    "    with tf.variable_scope('my_model', values=[inputs]) as sc:\n",
    "        end_points_collection = sc.original_name_scope + '_end_points'\n",
    "        # Collect outputs for conv2d, fully_connected and max_pool2d.\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],\n",
    "                            outputs_collections=[end_points_collection]):\n",
    "            net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')\n",
    "            net = slim.max_pool2d(net, [5, 5], 5, scope='pool1')\n",
    "            net = slim.max_pool2d(net, [3, 3], 2, scope='pool2')\n",
    "            end_points = slim.utils.convert_collection_to_dict(end_points_collection)\n",
    "            return net, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, wrap the model in the TensorFlow-slim wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from candidate_models.models.implementations.tensorflow_slim import TensorflowSlimModel\n",
    "# requires you to have TF slim installed: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "\n",
    "class MyModelWrapper(TensorflowSlimModel):\n",
    "    def _create_inputs(self, batch_size, image_size):\n",
    "        inputs = tf.placeholder(dtype=tf.float32, shape=[batch_size, image_size, image_size, 3])\n",
    "        preprocess_image = vgg_preprocessing.preprocess_image\n",
    "        return tf.map_fn(lambda image: preprocess_image(tf.image.convert_image_dtype(image, dtype=tf.uint8),\n",
    "                                                        image_size, image_size), inputs)\n",
    "\n",
    "    def _create_model(self, inputs):\n",
    "        return _create_tf_model(inputs)\n",
    "\n",
    "    def _restore(self, weights):\n",
    "        assert weights is None\n",
    "        init = tf.initialize_all_variables()\n",
    "        self._sess.run(init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, pass the model to the scoring method and pass an identifier (for caching results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(_variable=<xarray.Variable (aggregation: 1)>\narray([0.424662])\nAttributes:\n    raw:      Score(_variable=<xarray.Variable (benchmark: 2, layer: 1, aggre...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 1)>\narray(['center'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "from candidate_models import score_model\n",
    "\n",
    "score = score_model(model_identifier='test_tensorflow_slim', model=MyModelWrapper, \n",
    "                    layers=['my_model/pool2'], weights=None, pca_components=None)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns an aggregate Brain-Score of multiple benchmarks with individual benchmark scores in the raw values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring a model on neural data can be done in a single line using the `score_physiology` method.\n",
    "This call is agnostic of a specific model implementation, it will simply look up the model name \n",
    "in `neurality.models.models` and use the implementation defined there (also see `examples/model-activations.ipynb`).\n",
    "By default, the pre-defined layers of a model will be used to retrieve the activations, \n",
    "but you can also pass your own.\n",
    "Just like with the model implementations, the result of this method call will be cached \n",
    "so that it only needs to be computed once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(_variable=<xarray.Variable (aggregation: 1)>\narray([0.60981])\nAttributes:\n    raw:      Score(_variable=<xarray.Variable (benchmark: 2, layer: 7, aggre...,_coords=OrderedDict([('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 1)>\narray(['center'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "from candidate_models import score_model\n",
    "\n",
    "score = score_model(model='alexnet')\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score aggregate and raw values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A score typically comes with an estimate of the center (e.g. mean) and error (e.g. standard error of the mean).\n",
    "These values are aggregations over splits and often neuroids.\n",
    "\n",
    "We can also retrieve the raw scores from the same object, using `.attrs['raw']`.\n",
    "For the Brain-Score, these contain scores for individual layers on individual benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(_variable=<xarray.Variable (benchmark: 2, layer: 7, aggregation: 2)>\narray([[[0.376876, 0.006124],\n        [0.512881, 0.003886],\n        [0.537398, 0.003578],\n        [0.548949, 0.003162],\n        [0.588876, 0.00288 ],\n        [0.559185, 0.003265],\n        [0.51445 , 0.002867]],\n\n       [[0.625188, 0.004513],\n        [0.630744, 0.004208],\n        [0.626941, 0.003936],\n        [0.574096, 0.00437 ],\n        [0.48632 , 0.006042],\n        [0.425624, 0.005558],\n        [0.347992, 0.008119]]])\nAttributes:\n    raw:      <xarray.DataAssembly (benchmark: 2, layer: 7, split: 10, neuroi...,_coords=OrderedDict([('benchmark', <xarray.IndexVariable 'benchmark' (benchmark: 2)>\narray(['dicarlo.Majaj2015.IT', 'dicarlo.Majaj2015.V4'], dtype=object)), ('layer', <xarray.IndexVariable 'layer' (layer: 7)>\narray(['features.2', 'features.5', 'features.7', 'features.9', 'features.12',\n       'classifier.2', 'classifier.5'], dtype=object)), ('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "from candidate_models import score_model\n",
    "\n",
    "score = score_model(model='alexnet')\n",
    "benchmark_values = score.attrs['raw']\n",
    "print(benchmark_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw values are just another xarray object and as such, we can easily filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(_variable=<xarray.Variable (aggregation: 2)>\narray([0.625188, 0.004513])\nAttributes:\n    raw:      <xarray.DataAssembly (split: 10, neuroid: 256)>\\narray([[     n...,_coords=OrderedDict([('benchmark', <xarray.Variable ()>\narray('dicarlo.Majaj2015.V4', dtype='<U20')), ('layer', <xarray.Variable ()>\narray('features.2', dtype='<U10')), ('aggregation', <xarray.IndexVariable 'aggregation' (aggregation: 2)>\narray(['center', 'error'], dtype='<U6'))]),_name=None,_file_obj=None,_initialized=True)\n"
     ]
    }
   ],
   "source": [
    "print(benchmark_values.sel(benchmark='dicarlo.Majaj2015.V4', layer='features.2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For individual benchmarks, the raw values contain single correlations over neuroids and splits.\n",
    "Since Brain-Score's raw values are the values for individual benchmarks, they contain another raw values attribute which you can use to retrieve the per-split, per-neuroid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly (benchmark: 2, layer: 7, split: 10, neuroid: 256)>\narray([[[[0.368393, ...,      nan],\n         ...,\n         [0.361794, ...,      nan]],\n\n        ...,\n\n        [[0.344986, ...,      nan],\n         ...,\n         [0.310163, ...,      nan]]],\n\n\n       [[[     nan, ..., 0.375042],\n         ...,\n         [     nan, ..., 0.401341]],\n\n        ...,\n\n        [[     nan, ..., 0.515528],\n         ...,\n         [     nan, ..., 0.475515]]]])\nCoordinates:\n  * benchmark   (benchmark) object 'dicarlo.Majaj2015.IT' 'dicarlo.Majaj2015.V4'\n  * neuroid     (neuroid) MultiIndex\n  - neuroid_id  (neuroid) object 'Chabo_L_A_2_4' 'Chabo_L_A_3_3' ...\n  - arr         (neuroid) object 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' ...\n  - col         (neuroid) int64 4 3 5 0 1 2 3 4 5 6 2 3 5 0 1 2 3 4 5 6 1 2 ...\n  - hemisphere  (neuroid) object 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' ...\n  - subregion   (neuroid) object 'cIT' 'cIT' 'aIT' 'cIT' 'cIT' 'cIT' 'cIT' ...\n  - animal      (neuroid) object 'Chabo' 'Chabo' 'Chabo' 'Chabo' 'Chabo' ...\n  - y           (neuroid) float64 -1.0 -0.6 -0.6 -0.2 -0.2 -0.2 -0.2 -0.2 ...\n  - x           (neuroid) float64 -0.2 -0.6 0.2 -1.8 -1.4 -1.0 -0.6 -0.2 0.2 ...\n  - row         (neuroid) int64 2 3 3 4 4 4 4 4 4 4 5 5 5 6 6 6 6 6 6 6 7 7 ...\n  * layer       (layer) object 'classifier.2' 'classifier.5' 'features.12' ...\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9\n"
     ]
    }
   ],
   "source": [
    "raw_scores = benchmark_values.attrs['raw']\n",
    "print(raw_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
